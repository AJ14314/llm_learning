{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49aca6-69ad-4b92-9796-38a590d9027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44552637-c4be-4575-92b1-cf6e0de2e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain and Chroma and plotly\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df444eb6-2217-428e-845a-9603af8a1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "db_name = \"vector_db\"\n",
    "path = \"stock_analysis/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1dcec-b56a-41ea-bb25-35f97c1ba56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb8a25-5233-4487-9445-e4139212e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(path)\n",
    "print(\"folders \", folders)\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784944a-4aec-4805-9325-a1e96bd141c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd6334-703f-4e51-aae7-ade0601fed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7585b7-a8dc-4daa-8dcf-9b950facea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04b96e-d8e9-4e8b-852d-ee2de5d67e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d8228-cd27-4d84-8835-8f6d72b5ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our FAISS vectorstore!\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "total_vectors = vectorstore.index.ntotal\n",
    "dimensions = vectorstore.index.d\n",
    "\n",
    "print(f\"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fae321-b36b-4b11-bc3a-ef8643563b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework for FAISS\n",
    "vectors = []\n",
    "documents = []\n",
    "doc_types = []\n",
    "colors = []\n",
    "color_map = {'reports':'blue', 'defs':'green', 'contracts':'red', 'company':'orange'}\n",
    "\n",
    "for i in range(total_vectors):\n",
    "    vectors.append(vectorstore.index.reconstruct(i))\n",
    "    doc_id = vectorstore.index_to_docstore_id[i]\n",
    "    document = vectorstore.docstore.search(doc_id)\n",
    "    documents.append(document.page_content)\n",
    "    doc_type = document.metadata['doc_type']\n",
    "    doc_types.append(doc_type)\n",
    "    colors.append(color_map[doc_type])\n",
    "\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44664a40-84e6-4d10-be55-839f1749def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(25, len(vectors) - 1))\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b44f75-5f9b-4053-8b20-67ff6cd43bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42, perplexity=min(25, len(vectors) - 1))\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc8c57-5381-476d-8100-f696e7a701a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77091b-3c67-4815-a61b-0341c35af47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a chat with Gemini\n",
    "# llm  = ChatGoogleGenerativeAI(model=MODEL, temperature=0.7, google_api_key = GOOGLE_API_KEY)\n",
    "\n",
    "# # set up the conversation emory for the chat\n",
    "# memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# # the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "# # putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "# conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac014333-33e3-40ea-aa92-fb09355d7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Can you describe Insurellm in few sentences\"\n",
    "# result = conversation_chain.invoke({\"question\":query})\n",
    "# print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317a06e-1032-48ac-8ca5-a1c6205a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83b4e1-68a3-42f1-807c-da14eb886be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view = gr.ChatInterface(chat).launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b4597-6385-44b9-80f5-20b1fd25fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate what gets sent behind the scenes\n",
    "\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.7, google_api_key = GOOGLE_API_KEY)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "# query = \"Who received the prestigious IIOTY award in 2023?\"\n",
    "query = \"Why are you having only 4 stock data, I have given 600+ stocks data to you.\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b7276",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total chunks in vectorstore: {len(chunks)}\")\n",
    "print(f\"Total unique stocks: 621\")\n",
    "print(f\"Average chunks per stock: {len(chunks)/621:.2f}\")\n",
    "\n",
    "# Calculate recommended k value\n",
    "recommended_k = len(chunks)  # Start with maximum possible chunks\n",
    "print(f\"Recommended k value: {recommended_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48633782-9ac2-4681-b85d-8291f507b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.7, google_api_key = GOOGLE_API_KEY)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": int(recommended_k/2)})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91908697-8cb8-4fb4-a90b-ff5f38d114b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d15d3e-97c4-4098-a8fa-d364b20eea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e723b-0267-47e5-8cdd-0eb2d19aab1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
